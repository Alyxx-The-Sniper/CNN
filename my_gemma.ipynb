{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alyxx-The-Sniper/CNN/blob/main/my_gemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PXHL1Yl6CDO"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haxx698L6CDU"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPSfstBt6CDW"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWnpaTOt6CDX"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OlsLE1796CDY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lBN09c1tUlSV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install latest transformers for Gemma 3N\n",
        "!pip install --no-deps --upgrade timm # Only for Gemma 3N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMWlrRdzwgf"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "-Xbb0cuLzwgf",
        "outputId": "5b1644aa-1082-4d22-f7ea-65040f1b955e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-300965717.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastModel\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth currently only works on NVIDIA GPUs and Intel GPUs.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-300965717.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model, tokenizer = FastModel.from_pretrained(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"unsloth/gemma-3n-E4B-it\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_device_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth currently only works on NVIDIA GPUs and Intel GPUs."
          ]
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3n-E4B-it\",\n",
        "    dtype = None, # None for auto detection\n",
        "    max_seq_length = 4096, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "00L088fq5rdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "seZW9C5T5raT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yv0nQE4M5rXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Here"
      ],
      "metadata": {
        "id": "8jAKqhXHWU_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade gradio"
      ],
      "metadata": {
        "id": "hW6Tr1mQr-EB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============  Gradio demo ===============\n",
        "import gradio as gr, os, json, datetime, torch\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Helper: run Gemma-3n once and return the string (not stream)\n",
        "# ------------------------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def run_gemma(messages, max_new_tokens=1024):\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    gen_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=1.0,\n",
        "        top_p=0.95,\n",
        "        top_k=64,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    # Decode only the new tokens\n",
        "    new_ids = gen_ids[:, inputs['input_ids'].shape[1]:]\n",
        "    return tokenizer.decode(new_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Core workflow steps\n",
        "# ------------------------------------------------------------------\n",
        "def step_transcribe(audio):\n",
        "    \"\"\"\n",
        "    audio can be a string (path) or a tempfile object.\n",
        "    Coerce to str before sending to the model.\n",
        "    \"\"\"\n",
        "    if audio is None:\n",
        "        raise gr.Error(\"No audio provided or unclear audio.\")\n",
        "\n",
        "    # Gradio >= 4: audio can be a tuple (path, samplerate) -> take first element\n",
        "    if isinstance(audio, tuple):\n",
        "        audio_path = str(audio[0])\n",
        "    else:\n",
        "        audio_path = str(audio)\n",
        "\n",
        "    # -- Transcribe --\n",
        "    transcript = run_gemma([\n",
        "        {\"role\": \"user\",\n",
        "         \"content\": [\n",
        "             {\"type\": \"audio\", \"audio\": audio_path},\n",
        "             {\"type\": \"text\",  \"text\": \"Provide an accurate transcript of the audio.\"}\n",
        "         ]}\n",
        "    ], max_new_tokens=2048)\n",
        "\n",
        "    # -- Summarise --\n",
        "    summary = run_gemma([\n",
        "        {\"role\": \"user\",\n",
        "         \"content\": [\n",
        "             {\"type\": \"text\", \"text\": f\"Transcript:\\n{transcript}\\n\\nSummarise it concisely.\"}\n",
        "         ]}\n",
        "    ], max_new_tokens=2048)\n",
        "\n",
        "    return transcript, summary, gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def step_approve(choice, transcript, summary, feedback):\n",
        "    \"\"\"Handle approve / reject / regenerate.\"\"\"\n",
        "    feedback = feedback or \"\"  # <-- NEW: guard against None\n",
        "\n",
        "    if choice == \"✅ Approve\":\n",
        "        # save\n",
        "        stamp = datetime.datetime.utcnow().isoformat(timespec=\"seconds\")\n",
        "        fname = f\"saved_{stamp}.json\".replace(\":\", \"-\")\n",
        "        with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\"transcript\": transcript, \"summary\": summary}, f, ensure_ascii=False, indent=2)\n",
        "        return (\n",
        "            gr.update(visible=False),  # approval row\n",
        "            gr.update(visible=False),  # feedback row\n",
        "            f\"Saved to `{fname}` ✅\"\n",
        "        )\n",
        "    else:  # Rejected\n",
        "        if not feedback.strip():\n",
        "            return (\n",
        "                gr.update(visible=True),\n",
        "                gr.update(visible=True),\n",
        "                \"Please provide feedback first.\"\n",
        "            )\n",
        "        # regenerate\n",
        "        new_summary = run_gemma([\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": [\n",
        "                 {\"type\": \"text\", \"text\": f\"Transcript:\\n{transcript}\\n\\nPrevious summary was rejected. Summarize again with regards to this: {feedback}\"}\n",
        "             ]}\n",
        "        ], max_new_tokens=2048)\n",
        "        return (\n",
        "            gr.update(visible=True),   # keep approval block visible\n",
        "            gr.update(visible=True),  # hide feedback box again\n",
        "            new_summary\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Gradio UI\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# ==================== Voice-to-Summary with Approval ====================\n",
        "import gradio as gr, os, json, datetime, torch\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Helper: run Gemma-3n and return the decoded string\n",
        "# ------------------------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def run_gemma(messages, max_new_tokens=1024):\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    gen_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=1.0,\n",
        "        top_p=0.95,\n",
        "        top_k=64,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    new_ids = gen_ids[:, inputs['input_ids'].shape[1]:]\n",
        "    return tokenizer.decode(new_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Step 1: Transcribe + Summarize\n",
        "# ------------------------------------------------------------------\n",
        "def step_transcribe(audio_path):\n",
        "    if audio_path is None:\n",
        "        raise gr.Error(\"No audio provided. Please record or upload an audio file.\")\n",
        "\n",
        "    # No need to check for type, Gradio gives us a path string\n",
        "    print(f\"Processing audio file at: {audio_path}\")\n",
        "\n",
        "    # Now proceed with transcription\n",
        "    transcript = run_gemma([\n",
        "        {\"role\": \"user\",\n",
        "         \"content\": [\n",
        "             {\"type\": \"audio\", \"audio\": audio_path},\n",
        "             {\"type\": \"text\", \"text\": \"Provide an accurate transcript of the audio.\"}\n",
        "         ]}\n",
        "    ], max_new_tokens=2048)\n",
        "\n",
        "    summary = run_gemma([\n",
        "        {\"role\": \"user\",\n",
        "         \"content\": [\n",
        "             {\"type\": \"text\", \"text\": f\"Transcript:\\n{transcript}\\n\\nSummarise it concisely.\"}\n",
        "         ]}\n",
        "    ], max_new_tokens=2048)\n",
        "\n",
        "    # Make the approval buttons visible and hide the feedback box\n",
        "    return transcript, summary, gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Step 2/3: Approve or Reject/Improve\n",
        "# ------------------------------------------------------------------\n",
        "def step_approve(choice, transcript, summary, feedback):\n",
        "    feedback = feedback or \"\"\n",
        "\n",
        "    if choice == \"✅ Approve\":\n",
        "        stamp = datetime.datetime.utcnow().isoformat(timespec=\"seconds\").replace(\":\", \"-\")\n",
        "        fname = f\"saved_{stamp}.json\"\n",
        "        with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\"transcript\": transcript, \"summary\": summary}, f, ensure_ascii=False, indent=2)\n",
        "        return gr.update(visible=False), gr.update(visible=False), f\"Saved to `{fname}` ✅\"\n",
        "\n",
        "    # Rejected but no feedback\n",
        "    if not feedback.strip():\n",
        "        return gr.update(visible=True), gr.update(visible=True), \"Please provide feedback first.\"\n",
        "\n",
        "    # Rejected with feedback: regenerate summary\n",
        "    new_summary = run_gemma([\n",
        "        {\"role\": \"user\",\n",
        "         \"content\": [\n",
        "             {\"type\": \"text\", \"text\": f\"Transcript:\\n{transcript}\\n\\nPrevious summary was rejected. Summarize again with regards to this: {feedback}\"}\n",
        "         ]}\n",
        "    ], max_new_tokens=2048)\n",
        "\n",
        "    return new_summary, gr.update(visible=True), new_summary\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Gradio UI\n",
        "# ------------------------------------------------------------------\n",
        "with gr.Blocks(title=\"Voice-to-Summary with Approval\") as demo:\n",
        "    gr.Markdown(\"# 🎙️ Voice to Summary with Human-in-the-loop Approval\")\n",
        "\n",
        "    audio_in = gr.Audio(sources=[\"upload\", \"microphone\"], type=\"filepath\", label=\"Upload or record audio\")\n",
        "\n",
        "    btn_run = gr.Button(\"🚀 Transcribe & Summarise\", variant=\"primary\")\n",
        "\n",
        "    transcript_box = gr.Textbox(label=\"Transcript\", lines=10, interactive=False)\n",
        "    summary_box = gr.Textbox(label=\"Summary\", lines=5, interactive=True)\n",
        "    summary_state = gr.State()\n",
        "\n",
        "    with gr.Row(visible=True) as approval_row:\n",
        "        approve_btn = gr.Button(\"✅ Approve\")\n",
        "        reject_btn = gr.Button(\"❌ Reject / Improve\")\n",
        "\n",
        "    feedback_box = gr.Textbox(visible=False, label=\"Feedback / Instructions for new summary\", lines=3)\n",
        "    status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    # STEP 1: Transcribe & summarize\n",
        "    def first_run(audio):\n",
        "        transcript, summary, *ui = step_transcribe(audio)\n",
        "        return transcript, summary, *ui, summary\n",
        "\n",
        "    btn_run.click(\n",
        "        fn=first_run,\n",
        "        inputs=audio_in,\n",
        "        outputs=[transcript_box, summary_box, approval_row, feedback_box, summary_state]\n",
        "    )\n",
        "\n",
        "    # STEP 2: Approve\n",
        "    approve_btn.click(\n",
        "        fn=lambda t, s: step_approve(\"✅ Approve\", t, s, \"\"),\n",
        "        inputs=[transcript_box, summary_box],\n",
        "        outputs=[approval_row, feedback_box, status]\n",
        "    )\n",
        "\n",
        "    # STEP 3: Reject & regenerate\n",
        "    def reject_and_regen(t, s_old, f):\n",
        "        new_summary, feedback_vis, status_txt = step_approve(\"❌ Reject / Improve\", t, s_old, f)\n",
        "        return new_summary, feedback_vis, status_txt, new_summary\n",
        "\n",
        "    reject_btn.click(\n",
        "        fn=reject_and_regen,\n",
        "        inputs=[transcript_box, summary_state, feedback_box],\n",
        "        outputs=[summary_box, feedback_box, status, summary_state]\n",
        "    )\n",
        "\n",
        "demo.queue().launch(debug=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "65udgCyGWUKr",
        "outputId": "9e46050d-267a-4c7f-f37a-bc32c5c9f59f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://97c8bf55d4eaee0bac.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://97c8bf55d4eaee0bac.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing audio file at: /tmp/gradio/ea8170c1fdcfb128497c28e517695cc07c3b14e04f13e1839f2e1b003736dcd3/audio2.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 626, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 350, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2235, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1746, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 917, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2-2508491564.py\", line 220, in first_run\n",
            "    transcript, summary, *ui = step_transcribe(audio)\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2-2508491564.py\", line 149, in step_transcribe\n",
            "    transcript = run_gemma([\n",
            "                 ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2-2508491564.py\", line 118, in run_gemma\n",
            "    inputs = tokenizer.apply_chat_template(\n",
            "             ^^^^^^^^^\n",
            "NameError: name 'tokenizer' is not defined\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://97c8bf55d4eaee0bac.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}